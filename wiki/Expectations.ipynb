{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Expectations and the Law of Large Numbers\n",
    "\n",
    "##### Keywords: expectations, law of large numbers, lotus, distributions, probability"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "{:.no_toc}\n",
    "* \n",
    "{: toc}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expectations\n",
    "Expected values are used all over probability and statistics because they are one of the most natural ways of reducing an entire distribution down to a single, summary, quantity. For instance, the mean of a distribution is a particular expectation that captures where the values of the distribution falls on average, and the variance is another epectation that captures how spread out the distribution is on average.\n",
    "\n",
    "Expectations take in two inputs and return a single number as output. The inputs are a) a particular distribution $f(x)$ and b) a \"value\" function $v(x)$ that can be evaluated on the support of that distribution. The output is the sum of the value function evaluated at each point, weighted by the probability at that point. Becuase the probability across all points sums to 1, this is both a weighted average and a weighted sum: the total weight is 1.\n",
    "\n",
    "In symbols,\n",
    "$$E_f[v(X)] = \\sum_x v(x)\\,f(x).$$\n",
    "or in the continuous case,\n",
    "$$E_f[v(X)] = \\int v(x)\\,f(x) dx$$\n",
    "\n",
    "The $E_f[v(X)]$ notation compactly reminds us of the value function $v$ and the weight function $f$. It is unfortunately common to drop the $f$ from the notation, however, as authors often consider the distribution being used to be clear from context. The thought is that X is whatever random variable we're interested in, and $f$ will always be the ditribution of X.\n",
    "\n",
    "Throughout these notes we reserve the right to drop the $f$ subscript if we think the distribution being used is clear (or we are being lazy). Nevertheless, whenever you see an expectation, **YOU SHOULD CLARIFY**, what density/mass-function or distribution is it with respect to.\n",
    "\n",
    "It is also common to see the notation\n",
    "$$E[v(X)]= \\int v(x) dF(x),$$\n",
    "where $F$ is the cdf of $f$, and we've already dropped the subscript to test you. The idea is that this form makes it clear that you are weighing with probabilities from the distribution even in the continuous case. This form is also more general if you start working with measure theory.\n",
    "\n",
    "**Summarizing:** Expectations are a weighted sum that visits each point in the probability space. At each point, the expectation evaluates both the probability of that point arising and the value $v(x)$ assigned to that point. The overall calculation returns the weighted sum / weighted average of the function values, i.e. it returns a scalar value. Expecations thus nicely sumarize a particular distribution down to a single number with the interpretation of \"The average of v(x) over the entire distribution\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A common case: Mean a.k.a. E(x)\n",
    "Often when one says \"the\" expefcted value, the assumption is that $v(x)$ is the identity function so that $v(x)$ is just $x$. This is the particularly common case of finding the mean a.k.a. the expected value a.k.a the first moment of a distribuition.\n",
    "\n",
    "Thus, the expected value, or mean, or first moment, of X is defined to be\n",
    "$$\n",
    "E_{f}{X} = \\int x dF(x) = \n",
    "\\begin{cases}\n",
    "\\sum_x x f(x) & \\text{if X is discrete}\\\\\n",
    "\\int x f(x) dx & \\text{if X is continuous}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "### Example: A triangular distribution\n",
    "Consider the the distribution linear increasing distribution on [a,b] defined by $f(x)=\\frac{2}{(b-a)^2}(x-a)$. \n",
    "\n",
    "[TODO: scan image]\n",
    "\n",
    "The mean of this distribution is found via:\n",
    "$$E(X)=\\int{x\\frac{2}{(b-a)^2}(x-a)}$$\n",
    "$$E(X)=\\frac{2}{(b-a)^2}\\int{x(x-a)}$$\n",
    "$$E(X)=\\frac{2}{(b-a)^2}[{\\frac{1}{3}x^3-\\frac{1}{2}x^2a)}]_a^b$$\n",
    "$$E(X)=\\frac{2}{(b-a)^2}[\\frac{1}{3}b^3-\\frac{1}{2}b^2a - (\\frac{1}{3}a^3-\\frac{1}{2}a^3)] $$\n",
    "$$E(X)=\\frac{1}{3(b-a)^2}[2b^3-3b^2a +a^3] $$\n",
    "Conducting the long division yields:\n",
    "$$E(X)=\\frac{1}{3}(a+2b)$$\n",
    "So the mean is 2/3 of the way between a and b."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More complex: Variance\n",
    "The variance of a distribution is ALSO an expectaton, specifically:\n",
    "\n",
    "$$V_f[X] = E_f[(X-E_f[X])^2]$$.\n",
    "\n",
    "Interpreting this, we see that variance uses a score function of $v(x)=(x-\\mu)^2$ where $\\mu$ is the overall mean of the distribution in question. Thus, variance is scoring each point by how far it is from the mean, squared, and averaging over all points. Thus variance answers \"On average, how far is the distribution from its average\". (With \"far\" being squared distance: if we wanted we could use a value function that takes absolute value of $x-\\mu$ instead)\n",
    "\n",
    "### Example\n",
    "For the Bernoulli distribution $f(x)=p=constant$, and you are summing it over ones as opposed to 0's, so the mean is just p. The variance is $(1-p)^2\\times p +(-p)^2\\times (1-p) = p(1-p)(1-p+p) = p(1-p)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LOTUS: Another View, and Good News\n",
    "LOTUS or \"Law Of The Unconcious Staistician\" ties together two views of expecations: one thinking of expectations in terms of weighted averages of value functions and distributions, and one thinking of expectations as summarizing a random variable.\n",
    "\n",
    "We have discussed the weighted sum viewpoint, so let's spend a moment on the random variables viewpoint. In this setting you have some random variable $X$. $X$ must have a distribution/density, so let's call that $f(x)$. Conceptually, $X$ is the value observed in a [future] experiment and $f(x)dx$ is the probability of seing any particular value $x$. In this view, $E[X]=\\int{x\\,f(x)dx}$ is the mean of $X$. If you want to find the mean of $sin(X)$ well, that's a new random variable (call it Z) and it has a new distribution $g(z)$, and we'd calculate $E[Z]=\\int{z\\,g(z)dz}$\n",
    "\n",
    "Of course, in the weighted-sum viewpoint, we're just using the value function $sin(x)$ at each point of the distribution of $X$ and we would get $E[sin(X)]=\\int{sin(x)f(x)dx}$\n",
    "\n",
    "LOTUS tells us that the two views are equivalent. We could a) find the distribution of $Z$, and calculate $E(Z)=\\int{zg(z)dz}$ or we could skip finding the distribution of $Z$ and just do  $E[sin(X)]=\\int{sin(x)f(x)dx}$. We can work in terms of whichever distribution we have!\n",
    "\n",
    "**Theorem**:  \n",
    "if $Y = r(X)$, and $Y$ has (possibly unknown) distribution g(y),\n",
    "$$\n",
    "E[Y] = \\int{y\\,g(y)dy} = \\int{ r(x) f(x)dx}\n",
    "$$\n",
    "Warning: r(X) must be a deterministic transformation of X: we need a specific value of Y for each value of X.\n",
    "\n",
    "**Again, LOTUS says we get to work with whichever distribution we have our hands on**. If we want to calculate expectations for $sin(X)$ and $X^2$ and $3X-7$ we only need the distribution of $X$, even though each of the above quantities has very different distributions (e.g. the support for $sin(X)$ is limited to $[0,1]$ while $X^2$ is $[0,\\infty]$)\n",
    "\n",
    "**Example**:\n",
    "\n",
    "Specifically, let A be an event and let $r(x) = I_A (x)$ where   \n",
    "$I_A (x) = 1$ if $x \\in A$ and   \n",
    "$I_A (x) = 0$ if $x \\notin A$. \n",
    "That is, $r$ is 1 if A occurs and 0 otherwise\n",
    "\n",
    "Then:\n",
    "$$\n",
    "\\E{I_A (X)} = \\int I_A (x) dF(x) = \\int_A f_X (x) dx = p(X \\in A)\n",
    "$$\n",
    "\n",
    "This result links indicator functions (functions that answer \"Did the event occur?\") and probabilities. \n",
    "\n",
    "Specifically, the probability of an event can be found via the expected value of the indicator function. This result will become extremely useful as we learn that expectations that randomly just part of the space are often close to the true expectation. We will be able to approximate a difficult probabilty like \"The best hand in a game of poker is three of a kind\" by just running an experiment and scoring 1 when the event occurs and 0 otherwise, then averaging all the 1s and 0s we witnessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "celltoolbar": "Edit Metadata",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
