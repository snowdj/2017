{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Independence\n",
    "\n",
    "Two variables are said to be independent if the conditional probability of one given the other is just the probability of the first item: $$P(X|Y)=P(X)$$\n",
    "The above is enough to establish the other direction, $P(Y|X)=P(Y)$ so we don't need to worry about WHICH one is independent; they're both independent of each other.\n",
    "\n",
    "Interpreting the definition, by conditioning on a particular $Y$, we have learned nothing about $X$. That is: knowing the value of Y gives us no new insight into the value of $X$, the probability is still the original P(X), exactly like it was before we were given $Y$.\n",
    "\n",
    "\n",
    "### Simplifying the Joint\n",
    "Normally, a joint distribution factors via the \"and\" rule into conditional distributions:\n",
    "\n",
    "$$ P(X,Y) = P(X|Y)P(Y) $$\n",
    "\n",
    "If $X$ and $Y$ are independent, the joint factors nicely into marginals: $ P(X,Y) = P(X)P(Y) $ The converse is also true, and truly useful: if we have the marginals of two independent variables we can re-build the joint (and thus everything about them) by simply multiplying the marginals. This makes for a HECK of a data compression: we only have to store two marginal distributions instead of thier cartesian product.\n",
    "\n",
    "### Conditional Independence\n",
    "Two variables $X$ and $y$ and said to be conditionally independent of $Z$ if the following holds:\n",
    "\n",
    "$$ p(X,Y|Z) = p(X|Z) p(Y|Z) $$\n",
    "In words, this says that once Z is behind bars (i.e. we know its value), the usual factoriZation occurs with Z behind bars throughout. Conceptually it means that if we learn about Z, X and y become independent. \n",
    "\n",
    "Another way to write that $X$ and $y$ are conditionally independent of $Z$ is\n",
    "\n",
    "$$ p(X| Z, Y) = p(X|Z) $$\n",
    "In words: if X and Y are independent given Z and Z is behind bars we get to drop Y. Conceptually, conditional indepndence means that anything we can learn from about X from Y is already contained in Z. If we condition on $Z$, and now also learn about $y$, this is not going to change the probability of $X$.\n",
    "\n",
    "It is important to realize that conditional independence between $X$ and $y$ does not imply independence between $X$ and $Y$. It is very possible that $P(X|Y)$ does not become $P(X)$ until we have $Z$ given. See https://math.stackexchange.com/questions/23093/could-someone-explain-conditional-independence for an excellent set of examples\n",
    "\n",
    "### More variables\n",
    "Discussing independence of multiple variables is tricky. We can have pairwise independence of X and Y and Z without getting the nice joint distribution factoring result.\n",
    "\n",
    "Tread carefully.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
