{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mathlab (TM)\n",
    "\n",
    "##### Keywords: gradient descent, integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents\n",
    "{:.no_toc}\n",
    "* \n",
    "{: toc}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we will go through some examples of solving math questions in calculus, vectors and matrices. We'll revisit these at other points in the course, as needed, but hopefully this notebook provides you some practice, some cheatsheet, etc\n",
    "\n",
    "A lot of the material here is taken from Daume's \"Math for machine learning\" and  Bengio's book on deep learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Differential calculus\n",
    "\n",
    "The derivative of a function is the slope of the function at that point. For example, velocity is the derivative of the position.\n",
    "\n",
    "It's defined thus\n",
    "\n",
    "$$\\frac{df}{dx}\\Big{\\vert}_{x=x_0} = \\lim_{h \\to 0}\\frac{f(x_0 + h) - f(x_0)}{h}$$\n",
    "\n",
    "We'll abuse the notation $\\partial_x$ for derivative with respect to x for now, as its more concise.\n",
    "\n",
    "Some formulae:\n",
    "\n",
    "![](https://www.dropbox.com/s/ouihdyln5nneh90/Screenshot%202017-02-03%2002.41.02.png?dl=1)\n",
    "\n",
    "Let's try some functions. You will want to write the answer here and you can check it against the lab notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q1.** Differentiate $xe^{-x^2}$\n",
    "\n",
    "**Q2.** Differentiate  $\\frac{e^x + 1}{e^{-x}}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A function is said to be **convex** if it looks like a bowl. In other words, you are guaranteed a **global** minimum. Its second derivative must be $\\geq0$ everywhere. In multiple dimensions \"second derivtive is positive\" is replaced by \"The Hessian matrix (of all possible second derivatives) is positive semi-definite\".\n",
    "\n",
    "A **concave function** is an inverted bowl. It has a global maximum, and its second derivative must be negative everywhere.\n",
    "\n",
    "**Q3.** What kind of a function is $2x^2 - 3x + 1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Use in Optimization\n",
    "\n",
    "The first derivative of $f$ at a point $x_0$, if the derivative exists, describes the slope of $f$ function in an area around $x_0$. So if $\\frac{df}{dx}\\Big{\\vert}_{x=x_0}$ is positive, increasing the value of $x$ to something slightly bigger than $x_0$ should increase the value of $f$. The same statement holds for decreasing $x$ to decrease $f$\n",
    "\n",
    "**This means that anywhere the derivative exists and is non-zero cannot possibly be an maximum or minimum.** If the derivative is positive, there's a bigger point slightly to the right and a smaller point slightly to the left. If the derivative is negative, vice versa. *Thus, to find an optimum we only need to consider points where the derivative is zero or undefined.*\n",
    "\n",
    "Note that the derivative can be zero and not be an optimum: these are called \"saddle points\", as in the picture below. Also note that an undefined derivative does not ensure an optimum. We need additional tests to confirm that we've found a maximum/minimum.\n",
    "\n",
    "Critical points in 1D, from Bengio.\n",
    "\n",
    "![](https://www.dropbox.com/s/196s7o9cbrjbbjx/Screenshot%202017-02-03%2003.20.30.png?dl=1)\n",
    "\n",
    "As you might guess from the drawing, *a first derivative that changes from negative to zero to positive* or equivalently *a first derivative of 0 and second derivative that is positive **is** enough to prove that we've found a minimum.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Integral Calculus\n",
    "\n",
    "There are two separate concepts here. One is the anti-derivative, or \"find the function whose derivative I am\". The second concept is the area under the curve, also called the integral. The fundamental theorem of calculus tells us that the antiderivative is the integral. \n",
    "\n",
    "This diagram, taken from wikipedia, illustrates this theorem:\n",
    "\n",
    "![](https://upload.wikimedia.org/wikipedia/commons/e/e6/FTC_geometric.svg)\n",
    "\n",
    "The key idea is that the change in total area under the curve from a tiny change in $x$ is, essentially, the height of $f$ at x. So if we look at $F$, the accumulated area function, we can see that its derivative is $f$! $\\frac{dF}{dx}\\Big{\\vert}_{x}$ should tell us how much a tiny change in $x$ changes $F$, and we have that answer: plug just $x$ into $f$. Thus $f(x)$ is the function that tells us about the change in the value of $F$; it is the derivative of $F$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q4.** Integrate  $\\int_{-\\infty}^{\\infty} e^{-x^2}$\n",
    "\n",
    "**Q5.** What is the antiderivative of $x^2$; ie $\\int x^2 dx$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectors\n",
    "\n",
    "I am going to assume that you are familiar with vectors like  ${\\bf v}$, and their dot product ${\\bf v} \\cdot {\\bf w}$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Q6.** Write down the vector which has equal weight in all three directions, and is a unit vector (ie of length 1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can learn much more about vectors here http://www.feynmanlectures.caltech.edu/I_11.html ."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multidimensional Calculus\n",
    "\n",
    "This is where the partial symbol comes into its own. If $f$ is a function like $x^2 + y^2$, you can take derivatives with respect to both x and y, which gives us:\n",
    "\n",
    "$$\\frac{\\partial{f}}{\\partial{x}} = 2x$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can combine partials into a vector. This vector is known as the gradient vector.\n",
    "\n",
    "$$\\nabla f = (2x, 2y)$$\n",
    "\n",
    "An amazingly important property of the gradient is that 1) it is a vector 2) it points in the direction of greatest increase in the original function. The parallel here is that in 1 dimension there were only two options: the greatest increase comes from moving left (negative slope) or comes from moving right (positive slope). But brilliantly, this pattern holds in arbitrary dimensions: if we make a vector of the change resulting from a tiny step in each possible direction it points in the direction we should move to get the greatest increase.\n",
    "\n",
    "There is something of an economic flavor here: the entries of the gradient in some sense tell us how to spend our 'movement budget' to get the greatest increase in the original funciton. Spend more on directions with a lage gradient entry, run away from directions with a negative entry."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Q7.** Compute the gradient of $f(\\mathbf{w}) = \\mathbf{w}^T\\mathbf{w}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Jacobians\n",
    "\n",
    "*Bear in mind that gradients only exist for functions from $R^n\\rightarrow R$; that is, functions with only one output*. If a function maps, say $R^2\\rightarrow R^3$ or vice versa we end up with a 3x2 or 2x3 matrix of derivatives (respectively) called the Jacobian. \n",
    "\n",
    "(Note that becuase the gradient is written as a row vector, the jacobian expects to act on on a column vector placed to the right of it, so the \"input\" size is the second dimension of the matrix, not the first)\n",
    "\n",
    "**Example:**, if we take the function $[x,y,z]\\rightarrow[3x+y-z\\,,z^2]$ (mapping points in R^3 down to R^2) we'd get the following Jacobian matrix\n",
    "\n",
    "\\begin{equation*}\n",
    "\\begin{bmatrix}\n",
    "3&1&-1\\\\\n",
    "0&0&2z\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "The first column is the derivative of each component with respect to $x$, the secong column is the derivative with respect to $y$ and so on. Equivalently, the first row is the graident of the first coordinate of the maping ($3x+y-z$) and the second row is the gradient of the second coordinate of the mapping ($z^2$).\n",
    "\n",
    "If we look at a mapping from $R^n$ to itself we get a square matrix and it can make sense to ask how much the transformation streteched $R^n$ at any given point. **The determinant of the Jacobian matrix tells us the amount of stretching (e.g. stretched by a factor of $z-x$ at each point)**. This 'amount of stretching' measure is analgous to the value of the first derivative in one dimensional settings. The above matrix doesn't have a determinant becuase it isn't square and it doesn't make sense to ask how much stretching went on in a mapping from $R^3$ to $R^2$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Q8** We saw the Jacobian in class on thursday. Compute the 3-D jacobian for a tranformation from cartesian $=(x,y,z)$ co-ordinates to cylindrical ones $(r, \\phi, z)$, and find its determinant."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Hessian\n",
    "The Hessian matrix is a best thought of as generalizing second derivatives in the same way the gradient generalizes first derivatives. As such it is extremely useful for understanding the curvature of a function. Like the Gradient and unlike the Jacobian, the Hessian only applies to functions from $R^n\\rightarrow R$. If we have $R^n\\rightarrow R^m$ we could write down m different hessian matrices, one for each component, but this is rarely done.\n",
    "\n",
    "To build a Hessian, simply write down all of the possible second derivateves. For example, taking $[x,y,z]\\rightarrow[x+y^2z]$ we would get a gradient of:\n",
    "\n",
    "$$Grad = [1, 2yz, y^2]$$\n",
    "\n",
    "And a Hessian of:\n",
    "\\begin{equation*}\n",
    "\\begin{bmatrix}\n",
    "0&0&0\\\\\n",
    "0&2z&2y\\\\\n",
    "0&2y&0\n",
    "\\end{bmatrix}\n",
    "\\end{equation*}\n",
    "\n",
    "The row and column labels are each x, then y, then z, so the entry in 3,2 is $\\frac{\\partial{f}}{\\partial{z}\\partial{y}}$. Equivalenly, the first row is the partial derivatives of the gradeint with respect to x, and the second row is the partials of the gradient with repsect to y. The Hessian is typically symmetric. Formally, the matrix entires being continuous functions of x, y, and z is enough to ensure symetry.\n",
    "\n",
    "**Q9.** Calculate the Hessian of $$f(x) = x_1^2 + x_2^2$$. Make a 3D plot of this function to see what it looks like."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuity\n",
    "\n",
    "**Q10.** What is the derivative of $f(x) = \\vert x \\vert$. \n",
    "\n",
    "**Q11.** What is the derivative of the step function which is 0 for all x less than 0 and 1 otherwise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Taylor Series\n",
    "\n",
    "The taylor series expansion for a function about a point a is given as\n",
    "\n",
    "$$f(x) = \\sum_i  \\frac{f^{(n)}(x-a)}{n!}x^n$$\n",
    "\n",
    "where $f^{(n)}$ is the nth derivative.\n",
    "\n",
    "**Q11** Expand $e^x$ in a taylor series\n",
    "\n",
    "**Q12** Approximately calculate $1.05^5$ using a taylor series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
